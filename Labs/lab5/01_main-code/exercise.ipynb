{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14",
   "metadata": {},
   "source": [
    "# Chapter 5 - Exercises\n",
    "\n",
    "> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
   "metadata": {},
   "source": [
    "# Exercise 5.1: Temperature-scaled softmax scores and sampling probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5860ba9f-2db3-4480-b96b-4be1c68981eb",
   "metadata": {},
   "source": [
    "**Empirical Analysis of Token Sampling Frequencies Under Temperature Scaling**\n",
    "\n",
    "**Key Research Question: How does temperature-based scaling of the `softmax` probability distribution impact the sampling frequency of the specific lexical token `\"pizza\"`?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "Utilize the `print_sampled_tokens` function to:\n",
    "- Empirically examine token sampling probabilities\n",
    "- Analyze the impact of temperature scaling\n",
    "- Quantify the sampling occurrence of the `\"pizza\"` token\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Determine the precise sampling frequency of `\"pizza\"` across different temperature configurations\n",
    "- Critically evaluate the current computational approach to sampling frequency measurement\n",
    "- Explore potential methodological improvements for more efficient and accurate token sampling analysis\n",
    "\n",
    "*Key Investigative Parameters:*\n",
    "- Primary token of interest: `\"pizza\"`\n",
    "- Sampling method: Temperature-scaled `softmax` distribution\n",
    "- Computational tool: `print_sampled_tokens` function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa2b23c",
   "metadata": {},
   "source": [
    "### Résultats - Exercise 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80105c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher, and lower temperature\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8121ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Temperature: 1\n",
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n",
      "\n",
      "\n",
      "Temperature: 0.1\n",
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "985 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "15 x toward\n",
      "\n",
      "\n",
      "Temperature: 5\n",
      "165 x closer\n",
      "75 x every\n",
      "42 x effort\n",
      "239 x forward\n",
      "71 x inches\n",
      "46 x moves\n",
      "32 x pizza\n",
      "227 x toward\n",
      "103 x you\n"
     ]
    }
   ],
   "source": [
    "for i, probas in enumerate(scaled_probas):\n",
    "    print(\"\\n\\nTemperature:\", temperatures[i])\n",
    "    print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc890262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0430)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp5_idx = 2\n",
    "pizza_idx = 6\n",
    "\n",
    "scaled_probas[temp5_idx][pizza_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f731e",
   "metadata": {},
   "source": [
    "**Interprétation**  \n",
    "En divisant les logits par une température, on modifie directement la distribution softmax :  \n",
    "- Température faible (< 1) → distribution plus “pointue” → le modèle choisit plus souvent les tokens les plus probables.  \n",
    "- Température élevée (> 1) → distribution plus “plate” → davantage de diversité dans l’échantillonnage.  \n",
    "L’estimation de la probabilité du token spécifique pizza montre cet effet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510ffb0-adca-4d64-8a12-38c4646fd736",
   "metadata": {},
   "source": [
    "# Exercise 5.2: Different temperature and top-k settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884990db-d1a6-4c4e-8e36-2c1e4c1e67c7",
   "metadata": {},
   "source": [
    "**Empirical Investigation of Generative Language Model Sampling Parameters**\n",
    "\n",
    "**Key Research Question: How do variations in `temperature` and `top-k` sampling parameters influence the qualitative and probabilistic characteristics of token generation in stochastic language models?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "Conduct a systematic empirical exploration of:\n",
    "- Temperature scaling dynamics\n",
    "- Top-k probability truncation mechanisms\n",
    "- Generative output characteristics across different parameter configurations\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Identify contextual applications that benefit from lower `temperature` and `top-k` settings\n",
    "- Explore potential use cases preferring higher `temperature` and `top-k` configurations\n",
    "- Develop nuanced understanding of sampling parameter impact on generative outputs\n",
    "\n",
    "*Investigative Dimensions:*\n",
    "1. Low `temperature` and `top-k` Scenarios\n",
    "   - Potential applications\n",
    "   - Characteristics of generated outputs\n",
    "   - Contextual relevance\n",
    "\n",
    "2. High `temperature` and `top-k` Scenarios\n",
    "   - Potential applications\n",
    "   - Characteristics of generated outputs\n",
    "   - Contextual relevance\n",
    "\n",
    "*Recommended Experimental Protocol:*\n",
    "1. Systematically vary `temperature` and `top-k` parameters\n",
    "2. Meticulously document generative output characteristics\n",
    "3. Critically analyze observed variations\n",
    "4. Develop hypotheses about optimal parameter configurations for specific applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ee172",
   "metadata": {},
   "source": [
    "### Résultats - Exercise 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f83534a",
   "metadata": {},
   "source": [
    "**Interprétation**  \n",
    "- La température contrôle la “randomness” : plus elle est haute, plus la distribution est plate → sorties plus variées mais parfois moins cohérentes.  \n",
    "- Le top-k limite la génération aux k tokens les plus probables → permet de garder de la diversité tout en évitant des tokens trop improbables.  \n",
    "Normalement, temperature et top_k se règlent ensemble pour équilibrer créativité vs cohérence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35425d-529d-4179-a1c4-63cb8b25b156",
   "metadata": {},
   "source": [
    "# Exercise 5.3: Deterministic behavior in the decoding functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12229a2-1d52-46ff-b1e8-198f2e58a7d2",
   "metadata": {},
   "source": [
    "**Deterministic Token Generation: Parametric Strategies for Eliminating Stochastic Variability**\n",
    "\n",
    "**Key Research Question: What specific configuration parameters within the `generate` function can systematically eliminate randomness to ensure consistently reproducible generative outputs?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "*Investigate comprehensive strategies to:*\n",
    "- Suppress stochastic token generation mechanisms\n",
    "- Enforce deterministic computational behavior\n",
    "- Replicate the predictable output characteristics of `generate_simple`\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Identify all potential parameter combinations\n",
    "- Systematically neutralize probabilistic sampling variations\n",
    "- Establish deterministic generative protocol\n",
    "\n",
    "*Critical Configuration Parameters to Examine:*\n",
    "1. `temperature` scaling\n",
    "2. `top_k` pruning mechanism\n",
    "3. Random seed initialization\n",
    "4. Sampling strategy selection\n",
    "\n",
    "*Recommended Experimental Protocol:*\n",
    "1. Analyze individual parameter impacts\n",
    "2. Identify minimal configuration requirements\n",
    "3. Validate deterministic output generation\n",
    "4. Compare against `generate_simple` implementation\n",
    "\n",
    "*Computational Implications:*\n",
    "- Understanding stochastic suppression mechanisms\n",
    "- Insights into generative model controllability\n",
    "- Strategies for reproducible machine learning outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82610a0a",
   "metadata": {},
   "source": [
    "### Résultats - Exercise 5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f61ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_labs import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 256,# Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,       # Embedding dimension\n",
    "    \"n_heads\": 12,        # Number of attention heads\n",
    "    \"n_layers\": 12,       # Number of layers\n",
    "    \"drop_rate\": 0.1,     # Dropout rate\n",
    "    \"qkv_bias\": False     # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
    "model.eval();   # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69454ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_generate import generate, text_to_token_ids, token_ids_to_text\n",
    "from previous_labs import generate_text_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92150a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2b4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=None,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f4e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=None,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b6571c",
   "metadata": {},
   "source": [
    "**Interprétation**  \n",
    "Les deux fonctions (generate_text_simple et generate avec temperature=0) deviennent déterministes car elles choisissent systématiquement le token avec la probabilité maximale (équivalent à argmax).  \n",
    "En résultat on a donc à prompt identique, le texte généré reste le même à chaque exécution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0480e5-fb4e-41f8-a161-7ac980d71d47",
   "metadata": {},
   "source": [
    "# Exercise 5.4: Continued pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40044e8-a0f5-476c-99fd-489b999fd80a",
   "metadata": {},
   "source": [
    "**Continuation of Model Training: Stateful Resumption and Persistent Learning Dynamics**\n",
    "\n",
    "**Key Research Question: How can we effectively restore a machine learning model's training state across separate computational sessions, enabling seamless continuation of the pretraining process?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "Implement a comprehensive model and optimizer state restoration strategy involving:\n",
    "- Weight reconstruction\n",
    "- Optimizer state recovery\n",
    "- Resumption of training from previously interrupted state\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Demonstrate stateful model persistence\n",
    "- Execute additional training epoch using restored model configuration\n",
    "- Validate continuity of learning progression\n",
    "\n",
    "*Critical Procedural Steps:*\n",
    "1. Load previously saved model weights\n",
    "2. Reconstruct optimizer internal state\n",
    "3. Reinitiate training using `train_model_simple` function\n",
    "4. Complete one additional training epoch\n",
    "\n",
    "*Recommended Implementation Strategy:*\n",
    "- Utilize precise weight and optimizer state loading mechanisms\n",
    "- Verify complete state restoration\n",
    "- Execute uninterrupted additional training epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a22676",
   "metadata": {},
   "source": [
    "### Résultats - Exercise 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1656c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_labs import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from previous_labs import create_dataloader_v1\n",
    "\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://huggingface.co/datasets/DarwinAnim8or/the-verdict/blob/main/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4e70eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.271, Val loss 6.545\n",
      "Ep 1 (Step 000005): Train loss 0.244, Val loss 6.614\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "from gpt_train import train_model_simple\n",
    "\n",
    "num_epochs = 1\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108dc900",
   "metadata": {},
   "source": [
    "**Interprétation**  \n",
    "L’idée est de faire du continued pretraining : on repart d’un modèle existant qu'on a vu dans le lab5 et on continue l’entraînement.  \n",
    "Donc logiquement on s’attend à voir la loss diminuer et la génération devenir progressivement plus cohérente, même après peu d’epochs, car le modèle adapte ses poids au style et au vocabulaire du dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3384e788-f5a1-407c-8dd1-87959b75026d",
   "metadata": {},
   "source": [
    "# Exercise 5.5: Training and validation set losses of the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1140b-2027-4156-8d19-600ac849edbe",
   "metadata": {},
   "source": [
    "**Comparative Loss Assessment: Pretrained Model Performance on Specialized Textual Domain**\n",
    "\n",
    "**Key Research Question: What are the comparative training and validation set losses when applying a pretrained OpenAI `GPTModel` to the \"The Verdict\" dataset?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "Conduct a comprehensive loss evaluation involving:\n",
    "- Model weight initialization from pretrained OpenAI configuration\n",
    "- Computational loss calculation across training and validation datasets\n",
    "- Quantitative performance assessment in domain-specific context\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Determine precise loss metrics for training dataset\n",
    "- Calculate validation set loss\n",
    "- Interpret performance characteristics of pretrained model on specialized textual domain\n",
    "\n",
    "*Critical Computational Procedures:*\n",
    "1. Load pretrained OpenAI `GPTModel` weights\n",
    "2. Prepare \"The Verdict\" dataset\n",
    "3. Compute training set loss\n",
    "4. Compute validation set loss\n",
    "5. Comparative loss analysis\n",
    "\n",
    "*Investigative Parameters:*\n",
    "- Model: Pretrained OpenAI `GPTModel`\n",
    "- Dataset: \"The Verdict\"\n",
    "- Metrics: Training and validation loss measurements\n",
    "\n",
    "*Recommended Analytical Approach:*\n",
    "- Implement precise loss computation\n",
    "- Validate computational methodology\n",
    "- Critically interpret loss metric implications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb5f88",
   "metadata": {},
   "source": [
    "### Résultats - Exercise 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7890552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_labs import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "206fcaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03ba1839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8401562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_generate import load_weights_into_gpt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from previous_labs import create_dataloader_v1\n",
    "\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://huggingface.co/datasets/DarwinAnim8or/the-verdict/blob/main/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30ce18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.754756662580702\n",
      "Validation loss: 3.559627056121826\n"
     ]
    }
   ],
   "source": [
    "from gpt_train import calc_loss_loader\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
    "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4180322b",
   "metadata": {},
   "source": [
    "**Interprétation**  \n",
    "Ici, on compare les pertes train/validation du modèle pré-entraîné.  \n",
    "- Si les deux pertes sont proches et faibles : le modèle généralise correctement sur des séquences similaires.  \n",
    "- Si la perte train est nettement plus basse que la validation : cela peut indiquer un début de sur-apprentissage (overfitting) sur le texte d’entraînement.  \n",
    "On vois finalement que c'est donc pas mal dans notre cas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a76a1e0-9635-480a-9391-3bda7aea402d",
   "metadata": {},
   "source": [
    "# Exercise 5.6: Trying larger models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d313f4-0038-4bc9-a340-84b3b55dc0e3",
   "metadata": {},
   "source": [
    "**Comparative Generative Analysis: Scale and Performance Variations in GPT-2 Model Architectures**\n",
    "\n",
    "**Key Research Question: How do generative text characteristics vary across different GPT-2 model scales, specifically comparing the 124 million and 1,558 million parameter configurations?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "Conduct a systematic comparative investigation of:\n",
    "- Generative text quality\n",
    "- Semantic coherence\n",
    "- Linguistic complexity\n",
    "- Contextual understanding\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Empirically assess generative performance across model scales\n",
    "- Identify qualitative differences in text generation\n",
    "- Explore the relationship between model parameter count and generative capabilities\n",
    "\n",
    "*Comparative Model Configurations:*\n",
    "1. Smaller Model: **124 million parameters**\n",
    "2. Larger Model: **1,558 million parameters**\n",
    "\n",
    "*Investigative Dimensions:*\n",
    "- Textual coherence\n",
    "- Semantic precision\n",
    "- Contextual relevance\n",
    "- Linguistic nuance\n",
    "- Complexity of generated content\n",
    "\n",
    "*Experimental Protocol:*\n",
    "1. Generate text samples using both model configurations\n",
    "2. Conduct qualitative comparative analysis\n",
    "3. Assess generative performance across multiple dimensions\n",
    "4. Document observable variations in text generation characteristics\n",
    "\n",
    "*Recommended Analytical Approach:*\n",
    "- Utilize consistent generation parameters\n",
    "- Employ multiple generation trials\n",
    "- Implement rigorous qualitative assessment\n",
    "- Develop comprehensive comparative framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f221cc71",
   "metadata": {},
   "source": [
    "### Résultats - Exercise 5.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2effad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_labs import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f389eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from gpt_generate import load_weights_into_gpt\n",
    "\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-xl (1558M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
    "load_weights_into_gpt(gpt, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bddf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_generate import generate, text_to_token_ids, token_ids_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557e6465",
   "metadata": {},
   "source": [
    "**Interprétation**  \n",
    "Cet exercice montre l’impact de la taille du modèle : en général, plus le modèle est grand (plus de couches / têtes / dimension d’embedding), plus il a de capacité et peut obtenir une loss plus faible sur le même dataset.  \n",
    "En contrepartie, le coût de calcul augmente fortement (temps d’entraînement, VRAM/RAM). \n",
    "L’objectif est donc de trouver un compromis entre performance et ressources disponibles, par eexemple moi je n'ai pas pu le faire tourner sur mon PC en local car trop gros et long, je suis donc passer par Collab.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
