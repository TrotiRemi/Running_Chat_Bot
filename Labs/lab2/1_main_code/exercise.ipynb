{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab88d307-61ba-45ef-89bc-e3569443dfca",
   "metadata": {},
   "source": [
    "# Chapter 2 - Lab 1a - Exercise\n",
    "> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f678e62-7bcb-4405-86ae-dce94f494303",
   "metadata": {},
   "source": [
    "# Exercise 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fde9a6",
   "metadata": {},
   "source": [
    "### Exploring Byte Pair Encoding (BPE) Tokenization with Unknown Words\n",
    "\n",
    "In this exercise, you will explore how the Byte Pair Encoding (BPE) tokenizer from the `tiktoken` library processes unknown words. BPE is a subword tokenization technique that constructs its vocabulary by iteratively merging frequent sequences of characters or subwords. This approach allows BPE tokenizers to handle previously unseen words by decomposing them into smaller, known subunits.\n",
    "\n",
    "### Objective\n",
    "Answer the following questions based on your experimentation with the tokenizer:\n",
    "\n",
    "1. How does the BPE tokenizer decompose the input phrase **\"Akwirw ier\"** into token IDs?\n",
    "2. What are the subwords or characters corresponding to each token ID in the tokenized output?\n",
    "3. Can the tokenizer's decoding process successfully reconstruct the original input phrase **\"Akwirw ier\"** from the token IDs? Why or why not?\n",
    "\n",
    "### Theoretical Background\n",
    "Byte Pair Encoding begins with a minimal vocabulary of single characters, such as **\"a,\" \"b,\" \"c,\"** and so on. The tokenizer builds upon this base by iteratively merging frequently co-occurring characters into subwords, and subsequently merging frequent subwords into complete words. The merging process is guided by a frequency threshold or cutoff. \n",
    "\n",
    "For example:\n",
    "- In the initial stage, the character **\"d\"** and **\"e\"** might frequently appear together in a corpus. The tokenizer merges these characters into the subword **\"de\"** if their co-occurrence exceeds the frequency cutoff.\n",
    "- This subword then becomes part of the tokenizer's vocabulary and is used to tokenize words where it occurs, such as **\"define,\" \"depend,\" \"made,\"** and **\"hidden.\"**\n",
    "\n",
    "This hierarchical merging enables the BPE tokenizer to strike a balance between granularity and generalization, efficiently encoding both common words and rare or unknown words by breaking them into smaller units.\n",
    "\n",
    "### Task Steps\n",
    "\n",
    "1. **Tokenization**:\n",
    "   - Use the `tiktoken` BPE tokenizer to process the unknown input string **\"Akwirw ier.\"**\n",
    "   - Print the token IDs generated for this input.\n",
    "\n",
    "2. **Subword Decoding**:\n",
    "   - For each token ID in the resulting list, use the tokenizer's `decode` function to convert the ID back into its corresponding subword or character.\n",
    "\n",
    "3. **Reconstruction**:\n",
    "   - Apply the `decode` method to the entire list of token IDs to reconstruct the original input string. Verify whether the reconstructed string matches the initial input, **\"Akwirw ier.\"**\n",
    "\n",
    "\n",
    "\n",
    "### Questions - Exercise 2.1\n",
    "1. What sequence of token IDs does the BPE tokenizer generate for the input **\"Akwirw ier\"?**\n",
    "2. What subwords or characters correspond to each token ID in the sequence?\n",
    "3. Does the reconstructed output from the token IDs match the original input? Explain your observations and reasoning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da5d2a3",
   "metadata": {},
   "source": [
    "### Résultats - Exercise 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b2fd85",
   "metadata": {},
   "source": [
    "#### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ce849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6fe0061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c282bcb",
   "metadata": {},
   "source": [
    "**Interprétation**  \n",
    "La liste integers représente la phrase encodée en token IDs GPT-2.  \n",
    "Le fait d’avoir plusieurs IDs pour un seul mot signifie que le tokenizer utilise du subword tokenization (BPE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4da7e22",
   "metadata": {},
   "source": [
    "#### 2. Subword Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c328b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33901 -> Ak\n",
      "86 -> w\n",
      "343 -> ir\n",
      "86 -> w\n",
      "220 ->  \n",
      "959 -> ier\n"
     ]
    }
   ],
   "source": [
    "for i in integers:\n",
    "    print(f\"{i} -> {tokenizer.decode([i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1281e0e9",
   "metadata": {},
   "source": [
    "**Interprétation**  \n",
    "Chaque ID correspond à un sous-morceau (subword) du texte.  \n",
    "On observe que les mots rares/inconnus sont découpés en petits fragments (souvent quelques lettres), ce qui évite les problèmes d’out-of-vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75bfa27",
   "metadata": {},
   "source": [
    "#### 3. Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1c46f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Akwirw ier'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([33901, 86, 343, 86, 220, 959])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba8ca3",
   "metadata": {},
   "source": [
    "**Interprétation**  \n",
    "Même si le mot \"Akwirw\" n’existe pas dans le vocabulaire, BPE le découpe en sous-tokens connus (ex: morceaux de lettres fréquents).  \n",
    "Le décodage des IDs puis la reconstruction montre que le tokenizer peut représenter n’importe quel texte sans avoir besoin d’un token < unk >, car il retombe sur des *subwords*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e5034a-95ed-46d8-9972-589354dc9fd4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde21e07",
   "metadata": {},
   "source": [
    "**Exercise: Exploring Data Loader Behavior with Different Parameters**\n",
    "\n",
    "Certainly! Here's the exercise rewritten in the same structured style as the first one, ensuring clarity and consistency:\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise: Exploring Data Loader Behavior with Different Parameter Configurations**\n",
    "\n",
    "In this exercise, you will investigate how the parameters of a data loader—such as `max_length`, `stride`, and `batch_size`—affect the preparation of input-output pairs for training large language models (LLMs). By experimenting with these settings, you will gain a practical understanding of their impact on the data batching process and their implications for model training.\n",
    "\n",
    "### Objective\n",
    "You will:\n",
    "1. Observe how the data loader generates input-output pairs with different configurations of `max_length` and `stride`.\n",
    "2. Analyze how increasing the batch size changes the structure of the data and discuss the tradeoffs involved.\n",
    "3. Experiment with a batch size greater than 1 to understand how it impacts memory usage and input-output organization.\n",
    "\n",
    "---\n",
    "\n",
    "### Theoretical Background\n",
    "\n",
    "A data loader processes raw text data into smaller sequences suitable for training LLMs. Key parameters that influence its behavior are:\n",
    "\n",
    "1. **`max_length`**: Specifies the maximum sequence length for each input-output pair. Shorter sequences may simplify computation but can limit the context available to the model.\n",
    "   \n",
    "2. **`stride`**: Determines the step size for sliding the window over the text when creating sequences. A smaller stride increases overlap between sequences, leading to more redundancy. A larger stride reduces overlap, ensuring more unique coverage of the dataset.\n",
    "\n",
    "3. **`batch_size`**: Controls the number of sequences in a batch:\n",
    "   - **Small batches** (e.g., `batch_size=1`) are easier to process and require less memory. However, they can produce noisier gradient updates during training.\n",
    "   - **Larger batches** improve gradient stability but require more memory and computational power. This parameter is an important hyperparameter to tune during training.\n",
    "\n",
    "These parameters are central to efficient and effective preprocessing of data for training deep learning models.\n",
    "\n",
    "---\n",
    "\n",
    "### Task Steps\n",
    "\n",
    "1. **Experimenting with `max_length` and `stride`**:\n",
    "   - Run the data loader with two configurations:\n",
    "     - `max_length=2` and `stride=2`\n",
    "     - `max_length=8` and `stride=2`\n",
    "   - Observe the structure of the input-output pairs for each configuration and note how they differ.\n",
    "\n",
    "2. **Increasing Batch Size**:\n",
    "   - Experiment with a batch size of 8 using the following configuration:\n",
    "     ```python\n",
    "     dataloader = create_dataloader_v1(\n",
    "         raw_text, batch_size=8, max_length=4, stride=4,\n",
    "         shuffle=False\n",
    "     )\n",
    "     data_iter = iter(dataloader)\n",
    "     inputs, targets = next(data_iter)\n",
    "     print(\"Inputs:\\n\", inputs)\n",
    "     print(\"\\nTargets:\\n\", targets)\n",
    "     ```\n",
    "   - Examine the resulting `inputs` and `targets`. Consider how the data is structured when `batch_size` is increased compared to a batch size of 1.\n",
    "\n",
    "3. **Avoiding Overlap**:\n",
    "   - Analyze the effect of a `stride=4` setting. Note that this value ensures no overlap between sequences within a batch, minimizing redundancy and reducing the risk of overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### Questions - Exercise 2.2\n",
    "\n",
    "1. How do changes in `max_length` and `stride` affect the input-output mappings produced by the data loader?  \n",
    "2. What differences do you observe in the data when using a batch size of 8 compared to a batch size of 1?  \n",
    "3. How does using a larger stride (e.g., `stride=4`) influence the coverage of the dataset and the overlap between sequences?  \n",
    "\n",
    "---\n",
    "\n",
    "### Example Output\n",
    "\n",
    "Using the configuration:\n",
    "```python\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4,\n",
    "    shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)\n",
    "```\n",
    "\n",
    "**Inputs**:\n",
    "```plaintext\n",
    "tensor([[   40,   367,  2885,  1464],\n",
    "        [ 1807,  3619,   402,   271],\n",
    "        [10899,  2138,   257,  7026],\n",
    "        [15632,   438,  2016,   257],\n",
    "        [  922,  5891,  1576,   438],\n",
    "        [  568,   340,   373,   645],\n",
    "        [ 1049,  5975,   284,   502],\n",
    "        [  284,  3285,   326,    11]])\n",
    "```\n",
    "\n",
    "**Targets**:\n",
    "```plaintext\n",
    "tensor([[  367,  2885,  1464,  1807],\n",
    "        [ 3619,   402,   271, 10899],\n",
    "        [ 2138,   257,  7026, 15632],\n",
    "        [  438,  2016,   257,   922],\n",
    "        [ 5891,  1576,   438,   568],\n",
    "        [  340,   373,   645,  1049],\n",
    "        [ 5975,   284,   502,   284],\n",
    "        [ 3285,   326,    11,   287]])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Learning Outcomes\n",
    "\n",
    "By completing this exercise, you should:\n",
    "1. Understand how varying `max_length` and `stride` impacts the input-output pairs produced by the data loader.\n",
    "2. Appreciate the tradeoffs involved in choosing different batch sizes for training deep learning models.\n",
    "3. Gain insight into how stride settings can minimize redundancy and optimize dataset utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578d45d5",
   "metadata": {},
   "source": [
    "### Résultats - Exercise 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee4bc9c",
   "metadata": {},
   "source": [
    "#### 1. Experimenting with max_length and stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3899af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader(txt, batch_size=4, max_length=256, stride=128):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded_text = tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21b8a050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  40,  367],\n",
       "        [2885, 1464],\n",
       "        [1807, 3619],\n",
       "        [ 402,  271]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = create_dataloader(raw_text, batch_size=4, max_length=2, stride=2)\n",
    "\n",
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "    break\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c107914",
   "metadata": {},
   "source": [
    "**Interprétation**  \n",
    "- max_length=2 → chaque séquence d’entrée contient 2 tokens (très court).  \n",
    "- stride=2 → on avance de 2 tokens à chaque fois, donc pas de recouvrement ici (overlap nul).  \n",
    "Résultat : beaucoup de petits exemples, utiles pour illustrer le mécanisme mais peu de contexte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73f8368b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],\n",
       "        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138],\n",
       "        [ 1807,  3619,   402,   271, 10899,  2138,   257,  7026],\n",
       "        [  402,   271, 10899,  2138,   257,  7026, 15632,   438]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = create_dataloader(raw_text, batch_size=4, max_length=8, stride=2)\n",
    "\n",
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "    break\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60906efa",
   "metadata": {},
   "source": [
    "**Interprétation**  \n",
    "- max_length=8 → plus de contexte (8 tokens) par exemple.  \n",
    "- stride=2 → on avance de seulement 2 tokens à chaque fenêtre, donc fort recouvrement (overlap).  \n",
    "Résultat : plus d’exemples générés (fenêtres glissantes), mais une partie des tokens apparaît plusieurs fois dans des contextes légèrement décalés.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ac4ce4",
   "metadata": {},
   "source": [
    "#### 2. Increasing batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "871e44e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader(raw_text, batch_size=8, max_length=4, stride=4)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d581de5",
   "metadata": {},
   "source": [
    "**Interprétation**  \n",
    "- batch_size=8 → 8 séquences en parallèle dans un batch (plus “efficace” pour le GPU).  \n",
    "- stride=4 == max_length → pas de overlap entre fenêtres successives.  \n",
    "En général, stride=max_length permet d’éviter les doublons, mais peut “rater” certains alignements potentiellement utiles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a239c",
   "metadata": {},
   "source": [
    "#### 3. Avoiding Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79874dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],\n",
       "        [ 1807,  3619,   402,   271, 10899,  2138,   257,  7026],\n",
       "        [10899,  2138,   257,  7026, 15632,   438,  2016,   257],\n",
       "        [15632,   438,  2016,   257,   922,  5891,  1576,   438]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = create_dataloader(raw_text, batch_size=4, max_length=8, stride=4)\n",
    "\n",
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "    break\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13841b",
   "metadata": {},
   "source": [
    "**Interprétation**  \n",
    "Ici stride=4 avec max_length=8 réduit le recouvrement par rapport à stride=2.  \n",
    "Plus le stride est grand, moins on répète les mêmes tokens dans plusieurs séquences.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
